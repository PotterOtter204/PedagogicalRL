train:
  number_of_problems_per_batch: 2  # Reduced from 16
  num_samples_per_problem: 2  # Reduced from 8
  per_device_train_batch_size: 1  # Batch size per GPU (with 4 GPUs, gradient_accumulation_steps = 2*2 / 1 / 4 = 1)
  gradient_checkpointing: true  # CRITICAL: Reduces memory usage during training
  deepspeed_config_path: "config/deepspeed/zero3_4GPU.yaml"  # Use ZeRO-3 for better memory efficiency

  learning_rate: 5e-7
  beta: 0.001
  mu: 2
  optimizer: "paged_adamw_8bit"  # More memory efficient optimizer

teacher_model:
  model_name_or_path: "unsloth/Qwen3-4B-Instruct-2507"
  vllm:
    temperature: 1.0
    max_length: 4000
    max_num_seqs: 512
    gpu_memory_utilization: 0.20  # Reduced from 0.30 - less memory per vLLM instance
    number_of_gpus_per_instance: 1  # Use 1 GPU per instance
    max_number_of_instances: -1
    load_and_unload: true
    use_v0: true
    enforce_eager: true

student_model:
  model_name_or_path: "unsloth/Qwen3-4B-Instruct-2507"
  vllm:
    temperature: 0.6
    max_length: 4000
    max_num_seqs: 512
    gpu_memory_utilization: 0.20  # Reduced from 0.30 - less memory per vLLM instance
    number_of_gpus_per_instance: 1  # Use 1 GPU per instance
    max_number_of_instances: -1
    load_and_unload: true
    use_v0: false

judge_model:
  model_name_or_path: "google/gemini-2.5-flash-lite-preview-09-2025"
  use_openrouter: true
  vllm:
    temperature: 0.6
    max_length: 12000  # ADD: Judge needs full context
    max_num_seqs: 512   # ADD: From default config
    gpu_memory_utilization: 0.325  # ADD: Not used for API but keep consistent
    number_of_gpus_per_instance: 4  # ADD: Not used for API but keep consistent
    max_number_of_instances: -1  # ADD: From default config
    load_and_unload: false  # ADD: Judge stays loaded (but you're using API so doesn't matter)
    use_v0: false  # ADD: From default config

reward_model:
  model_name_or_path: None # We only use judge-based pedagogical reward for language tutoring

huggingface:
  name: <huggingface_name>
  push_to_hub: false

logging:
  wandb: true
  wandb_project: train-rl
  wandb_run_name: Language-Tutor-Test-Run  # CHANGED: More descriptive
  run_group: language_tutor  # CHANGED: From "7b"
  wandb_tags: ["language_tutor", "test", "4b"]  # CHANGED: More relevant tags
  save_dir: checkpoints/language_tutor  # CHANGED: Separate checkpoint dir
  save_steps: 10

generation:
  teacher_prompt_path: "prompt_templates/teacher_language_prompt.txt"
  
  student_personas_prompts_paths:
    correct_student: "prompt_templates/personas/language_student_correct.txt"
    error_student: "prompt_templates/personas/language_student_error.txt"

  judges_rules_prompts_paths:
    pedagogical_eval: "prompt_templates/judges/language_tutor_eval.txt"
  
  extra_penalty_for_rejected_judges: 0.25 
  ignore_rejected_judge: true  # Soft penalty approach
  use_experimental_shared_memory: false
  forced_conversation_type: "GUIDED"
  number_student_attempts: 0
  max_turns: 3

dataset:
  train_datasets:
    - name_or_path: "rd211/Big-Math-RL-Verified-Filtered"
      split: "train"
      ratio: 1.0
  max_train_examples: 2
