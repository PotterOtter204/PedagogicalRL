oot@ec41299f5813:~/PedagogicalRL# bash start_rl_training.sh --config_file config/deepspeed/zero3_4GPU.yaml --config-name language_tutor
[start_rl_training.sh] Launching VLLM server...
curl: (7) Failed to connect to localhost port 8005 after 0 ms: Couldn't connect to server
[start_rl_training.sh] Waiting for VLLM server...
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
curl: (7) Failed to connect to localhost port 8005 after 0 ms: Couldn't connect to server
[start_rl_training.sh] Waiting for VLLM server...
INFO 11-07 23:41:33 [__init__.py:239] Automatically detected platform cuda.
curl: (7) Failed to connect to localhost port 8005 after 0 ms: Couldn't connect to server
[start_rl_training.sh] Waiting for VLLM server...
2025-11-07 23:41:38,336 | rank:0 | gpu_logger | INFO | GPU0: 447.8/23034.0 MB (1.9%) | RAM: 42.6/503.5 GB (8.5%) | Total GPUs: 4, using 1 per instance
INFO:     Started server process [19805]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8005 (Press CTRL+C to quit)
2025-11-07 23:41:38,628 | rank:0 | gpu_logger | INFO | GPU0: 447.8/23034.0 MB (1.9%) | RAM: 42.6/503.5 GB (8.5%) | Limiting number of instances to 1
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO:     127.0.0.1:55778 - "GET /docs HTTP/1.1" 200 OK
[start_rl_training.sh] VLLM server is up.
[start_rl_training.sh] About to run:
  accelerate   launch   --config_file   config/deepspeed/zero3_4GPU.yaml   train_rl.py   --config-name   language_tutor
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 11-07 23:41:44 [__init__.py:239] Automatically detected platform cuda.
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
W1107 23:41:48.374000 20046 torch/distributed/run.py:792] 
W1107 23:41:48.374000 20046 torch/distributed/run.py:792] *****************************************
W1107 23:41:48.374000 20046 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W1107 23:41:48.374000 20046 torch/distributed/run.py:792] *****************************************
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Worker on GPUs [0] initializing for task 'generate'...
`torch_dtype` is deprecated! Use `dtype` instead!
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 11-07 23:41:57 [__init__.py:239] Automatically detected platform cuda.
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 11-07 23:41:57 [__init__.py:239] Automatically detected platform cuda.
INFO 11-07 23:41:57 [__init__.py:239] Automatically detected platform cuda.
INFO 11-07 23:41:57 [__init__.py:239] Automatically detected platform cuda.
WARNING 11-07 23:41:59 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 11-07 23:41:59 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.3) with config: model='unsloth/Qwen3-4B-Instruct-2507', speculative_config=None, tokenizer='unsloth/Qwen3-4B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=unsloth/Qwen3-4B-Instruct-2507, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False,
INFO 11-07 23:42:00 [cuda.py:292] Using Flash Attention backend.
INFO 11-07 23:42:01 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 11-07 23:42:01 [model_runner.py:1110] Starting to load model unsloth/Qwen3-4B-Instruct-2507...
WARNING 11-07 23:42:01 [utils.py:76] Qwen3ForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.
INFO 11-07 23:42:01 [transformers.py:118] Using Transformers backend.
`torch_dtype` is deprecated! Use `dtype` instead!
2025-11-07 23:42:01,615 | rank:1 | gpu_logger | INFO | GPU1: 818.9/23034.0 MB (3.6%) | RAM: 49.5/503.5 GB (9.8%) | Loading datasets from [{'name_or_path': 'rd211/Big-Math-RL-Verified-Filtered', 'split': 'train', 'ratio': 1.0}]
INFO 11-07 23:42:01 [weight_utils.py:265] Using model weights format ['*.safetensors']
2025-11-07 23:42:01,640 | rank:3 | gpu_logger | INFO | GPU3: 798.9/23034.0 MB (3.5%) | RAM: 49.4/503.5 GB (9.8%) | Loading datasets from [{'name_or_path': 'rd211/Big-Math-RL-Verified-Filtered', 'split': 'train', 'ratio': 1.0}]
2025-11-07 23:42:01,645 | rank:2 | gpu_logger | INFO | GPU2: 818.9/23034.0 MB (3.6%) | RAM: 49.4/503.5 GB (9.8%) | Loading datasets from [{'name_or_path': 'rd211/Big-Math-RL-Verified-Filtered', 'split': 'train', 'ratio': 1.0}]
2025-11-07 23:42:01,676 | rank:0 | gpu_logger | INFO | GPU0: 9504.0/23034.0 MB (41.3%) | RAM: 49.4/503.5 GB (9.8%) | Loading datasets from [{'name_or_path': 'rd211/Big-Math-RL-Verified-Filtered', 'split': 'train', 'ratio': 1.0}]
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.41it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.55it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.63it/s]

INFO 11-07 23:42:03 [loader.py:447] Loading weights took 1.46 seconds
No validation datasets provided or an error occurred while loading them.
Key 'eval_datasets' not in 'DatasetConfig'
    full_key: dataset.eval_datasets
    reference_type=DatasetConfig
    object_type=DatasetConfig
2025-11-07 23:42:03,446 | rank:1 | gpu_logger | INFO | GPU1: 818.9/23034.0 MB (3.6%) | RAM: 49.5/503.5 GB (9.8%) | Loaded 2 training examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
[2025-11-07 23:42:03,449][datasets.arrow_dataset][WARNING] - num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
No validation datasets provided or an error occurred while loading them.
Key 'eval_datasets' not in 'DatasetConfig'
    full_key: dataset.eval_datasets
    reference_type=DatasetConfig
    object_type=DatasetConfig
2025-11-07 23:42:03,534 | rank:3 | gpu_logger | INFO | GPU3: 798.9/23034.0 MB (3.5%) | RAM: 49.5/503.5 GB (9.8%) | Loaded 2 training examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
[2025-11-07 23:42:03,536][datasets.arrow_dataset][WARNING] - num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
INFO 11-07 23:42:03 [model_runner.py:1146] Model loading took 7.5454 GiB and 2.401150 seconds
No validation datasets provided or an error occurred while loading them.
Key 'eval_datasets' not in 'DatasetConfig'
    full_key: dataset.eval_datasets
    reference_type=DatasetConfig
    object_type=DatasetConfig
2025-11-07 23:42:03,949 | rank:2 | gpu_logger | INFO | GPU2: 818.9/23034.0 MB (3.6%) | RAM: 49.5/503.5 GB (9.8%) | Loaded 2 training examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
[2025-11-07 23:42:03,952][datasets.arrow_dataset][WARNING] - num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 11-07 23:42:05 [worker.py:267] Memory profiling takes 1.01 seconds
INFO 11-07 23:42:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (22.06GiB) x gpu_memory_utilization (0.75) = 16.54GiB
INFO 11-07 23:42:05 [worker.py:267] model weights take 7.55GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 0.16GiB; the rest of the memory reserved for KV Cache is 8.80GiB.
INFO 11-07 23:42:05 [executor_base.py:112] # cuda blocks: 4002, # CPU blocks: 1820
INFO 11-07 23:42:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 31.27x
No validation datasets provided or an error occurred while loading them.
Key 'eval_datasets' not in 'DatasetConfig'
    full_key: dataset.eval_datasets
    reference_type=DatasetConfig
    object_type=DatasetConfig
2025-11-07 23:42:05,328 | rank:0 | gpu_logger | INFO | GPU0: 18614.0/23034.0 MB (80.8%) | RAM: 50.3/503.5 GB (10.0%) | Loaded 2 training examples
num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
[2025-11-07 23:42:05,331][datasets.arrow_dataset][WARNING] - num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:  50%|█████████████████████████████████████████████▌                                             | 1/2 [00:00<00:00,  1.13it/s]INFO 11-07 23:42:08 [llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 4.94 seconds
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.36s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.29s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.29s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.38s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.68s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.68s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.68s/it]
Loading checkpoint shards:  50%|█████████████████████████████████████████████▌                                             | 1/2 [00:06<00:06,  6.06s/it]2025-11-07 23:42:19,110 | rank:2 | gpu_logger | INFO | GPU2: 8488.9/23034.0 MB (36.9%) | RAM: 63.7/503.5 GB (12.6%) | Node ID: 0, Process ID: 2, Number of processes: 4
Error executing job with overrides: []
Traceback (most recent call last):
  File "/root/PedagogicalRL/train_rl.py", line 185, in main
    trainer = ClassroomGRPOTrainer(
              ^^^^^^^^^^^^^^^^^^^^^
  File "/root/PedagogicalRL/src/grpo/trainer.py", line 418, in __init__
    self.ref_model = prepare_deepspeed(self.ref_model, self.accelerator)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/trl/models/utils.py", line 390, in prepare_deepspeed
    model, *_ = deepspeed.initialize(model=model, config=config_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/__init__.py", line 188, in initialize
    config_class = DeepSpeedConfig(config, mpu, mesh_device=mesh_device)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 752, in __init__
    self._configure_train_batch_size()
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 940, in _configure_train_batch_size
    self._batch_assertion()
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 880, in _batch_assertion
    assert (train_batch > 0), f"Train batch size: {train_batch} has to be greater than 0"
            ^^^^^^^^^^^^^^^
AssertionError: Train batch size: 0 has to be greater than 0

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
2025-11-07 23:42:19,129 | rank:3 | gpu_logger | INFO | GPU3: 8468.9/23034.0 MB (36.8%) | RAM: 63.7/503.5 GB (12.6%) | Node ID: 0, Process ID: 3, Number of processes: 4
Error executing job with overrides: []
Traceback (most recent call last):
  File "/root/PedagogicalRL/train_rl.py", line 185, in main
    trainer = ClassroomGRPOTrainer(
              ^^^^^^^^^^^^^^^^^^^^^
  File "/root/PedagogicalRL/src/grpo/trainer.py", line 418, in __init__
    self.ref_model = prepare_deepspeed(self.ref_model, self.accelerator)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/trl/models/utils.py", line 390, in prepare_deepspeed
    model, *_ = deepspeed.initialize(model=model, config=config_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/__init__.py", line 188, in initialize
    config_class = DeepSpeedConfig(config, mpu, mesh_device=mesh_device)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 752, in __init__
    self._configure_train_batch_size()
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 940, in _configure_train_batch_size
    self._batch_assertion()
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 880, in _batch_assertion
    assert (train_batch > 0), f"Train batch size: {train_batch} has to be greater than 0"
            ^^^^^^^^^^^^^^^
AssertionError: Train batch size: 0 has to be greater than 0

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
2025-11-07 23:42:19,360 | rank:1 | gpu_logger | INFO | GPU1: 8488.9/23034.0 MB (36.9%) | RAM: 63.9/503.5 GB (12.7%) | Node ID: 0, Process ID: 1, Number of processes: 4
Error executing job with overrides: []
Traceback (most recent call last):
  File "/root/PedagogicalRL/train_rl.py", line 185, in main
    trainer = ClassroomGRPOTrainer(
              ^^^^^^^^^^^^^^^^^^^^^
  File "/root/PedagogicalRL/src/grpo/trainer.py", line 418, in __init__
    self.ref_model = prepare_deepspeed(self.ref_model, self.accelerator)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/trl/models/utils.py", line 390, in prepare_deepspeed
    model, *_ = deepspeed.initialize(model=model, config=config_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/__init__.py", line 188, in initialize
    config_class = DeepSpeedConfig(config, mpu, mesh_device=mesh_device)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 752, in __init__
    self._configure_train_batch_size()
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 940, in _configure_train_batch_size
    self._batch_assertion()
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 880, in _batch_assertion
    assert (train_batch > 0), f"Train batch size: {train_batch} has to be greater than 0"
            ^^^^^^^^^^^^^^^
AssertionError: Train batch size: 0 has to be greater than 0

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.97s/it]
2025-11-07 23:42:21,276 | rank:0 | gpu_logger | INFO | GPU0: 17284.0/23034.0 MB (75.0%) | RAM: 65.7/503.5 GB (13.0%) | Node ID: 0, Process ID: 0, Number of processes: 4
Error executing job with overrides: []
Traceback (most recent call last):
  File "/root/PedagogicalRL/train_rl.py", line 185, in main
    trainer = ClassroomGRPOTrainer(
              ^^^^^^^^^^^^^^^^^^^^^
  File "/root/PedagogicalRL/src/grpo/trainer.py", line 418, in __init__
    self.ref_model = prepare_deepspeed(self.ref_model, self.accelerator)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/trl/models/utils.py", line 390, in prepare_deepspeed
    model, *_ = deepspeed.initialize(model=model, config=config_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/__init__.py", line 188, in initialize
    config_class = DeepSpeedConfig(config, mpu, mesh_device=mesh_device)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 752, in __init__
    self._configure_train_batch_size()
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 940, in _configure_train_batch_size
    self._batch_assertion()
  File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/config.py", line 880, in _batch_assertion
    assert (train_batch > 0), f"Train batch size: {train_batch} has to be greater than 0"
            ^^^^^^^^^^^^^^^
AssertionError: Train batch size: 0 has to be greater than 0

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
W1107 23:42:21.728000 20046 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 20367 closing signal SIGTERM
W1107 23:42:21.729000 20046 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 20368 closing signal SIGTERM
W1107 23:42:21.729000 20046 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 20369 closing signal SIGTERM
E1107 23:42:22.044000 20046 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 3 (pid: 20370) of binary: /usr/bin/python3.12
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py", line 1220, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py", line 906, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_rl.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-07_23:42:21
  host      : ec41299f5813
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 20370)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
INFO 11-07 23:42:22 [worker.py:133] Sleep mode freed 21.06 GiB memory, 0.29 GiB memory is still in use.
INFO 11-07 23:42:22 [executor_base.py:210] It took 14.113150 seconds to fall asleep.
2025-11-07 23:42:22,794 | rank:0 | gpu_logger | INFO | GPU0: 736.9/23034.0 MB (3.2%) | RAM: 61.1/503.5 GB (12.1%) | All workers are now asleep.
2025-11-07 23:42:22,795 | rank:0 | gpu_logger | INFO | GPU0: 736.9/23034.0 MB (3.2%) | RAM: 61.1/503.5 GB (12.1%) | Total GPUs: 4, using 1 per instance  
root@ec41299f5813:~/PedagogicalRL# 2025-11-07 23:42:22,947 | rank:0 | gpu_logger | INFO | GPU0: 736.9/23034.0 MB (3.2%) | RAM: 60.7/503.5 GB (12.1%) | Limiting number of instances to 1
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 11-07 23:42:28 [__init__.py:239] Automatically detected platform cuda.
Worker on GPUs [3] initializing for task 'generate'...
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 11-07 23:42:42 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 11-07 23:42:42 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 11-07 23:42:49 [__init__.py:239] Automatically detected platform cuda.
INFO 11-07 23:42:53 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='unsloth/Qwen3-4B-Instruct-2507', speculative_config=None, tokenizer='unsloth/Qwen3-4B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=unsloth/Qwen3-4B-Instruct-2507, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 11-07 23:42:54 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0291bc78f0>
^C
root@ec41299f5813:~/PedagogicalRL# ^C
root@ec41299f5813:~/PedagogicalRL# INFO 11-07 23:42:54 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 11-07 23:42:54 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 11-07 23:42:54 [gpu_model_runner.py:1258] Starting to load model unsloth/Qwen3-4B-Instruct-2507...
WARNING 11-07 23:42:54 [utils.py:76] Qwen3ForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.
INFO 11-07 23:42:54 [transformers.py:118] Using Transformers backend.
`torch_dtype` is deprecated! Use `dtype` instead!
WARNING 11-07 23:42:54 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 11-07 23:42:55 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.32it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.58it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.66it/s]

INFO 11-07 23:42:57 [loader.py:447] Loading weights took 1.46 seconds
INFO 11-07 23:42:57 [gpu_model_runner.py:1273] Model loading took 7.5454 GiB and 2.725255 seconds
INFO 11-07 23:42:59 [kv_cache_utils.py:578] GPU KV cache size: 53,696 tokens
INFO 11-07 23:42:59 [kv_cache_utils.py:581] Maximum concurrency for 2,048 tokens per request: 26.22x
INFO 11-07 23:42:59 [core.py:162] init engine (profile, create kv cache, warmup model) took 2.32 seconds
INFO 11-07 23:43:00 [block_pool.py:264] Successfully reset prefix cache
^C
root@ec41299f5813:~/PedagogicalRL# ^C
root@ec41299f5813:~/PedagogicalRL# INFO 11-07 23:43:08 [gpu_worker.py:81] Sleep mode freed 15.76 GiB memory, 0.31 GiB memory is still in use.
INFO 11-07 23:43:08 [executor_base.py:210] It took 8.251864 seconds to fall asleep.
2025-11-07 23:43:08,360 | rank:0 | gpu_logger | INFO | GPU0: 736.9/23034.0 MB (3.2%) | RAM: 74.2/503.5 GB (14.7%) | All workers are now asleep.
